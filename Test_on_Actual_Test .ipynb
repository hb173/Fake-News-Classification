{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train on Actual Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PrU_p05NVAyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3422ca82-bdfe-4732-ce5d-411d97a4e759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import metrics\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow import metrics\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "so54abtlVNyi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# import a naive bayes\n",
        "NaiveBayes_model = pickle.load(\n",
        "    open(\n",
        "        \"/content/gdrive/My Drive/NLP Project/NB_model.pkl\",\n",
        "        \"rb\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# import the count vectorizer for making Bag of Words\n",
        "Count_Vectorizer = pickle.load(\n",
        "    open(\n",
        "        \"/content/gdrive/My Drive/NLP Project/CV_BOG.pkl\",\n",
        "        \"rb\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# import a Tokenizer for assigning a unique integer to one\n",
        "Tokenizer1 =  pickle.load(\n",
        "    open(\n",
        "        \"/content/gdrive/My Drive/NLP Project/Text_Vec.pkl\",\n",
        "        \"rb\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "YqpZn0LfWxgo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import neural network\n",
        "\n",
        "\n",
        "BiLSTM_Model = load_model(\n",
        "    \"/content/gdrive/My Drive/NLP Project/neural_network.h5\"\n",
        ")"
      ],
      "metadata": {
        "id": "d461A_bXXBzo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "fake_news = pd.read_csv(\"/content/gdrive/My Drive/NLP Project/Fake_news_data.csv\")"
      ],
      "metadata": {
        "id": "gZnm2D3iVQTu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_news_test = fake_news.iloc[14999:,:] # get the last 5808 rows a test data completely unseen"
      ],
      "metadata": {
        "id": "1BhVNKOAVS0H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_news_test = fake_news_test.dropna()  # place this above the two cells\n",
        "label = fake_news_test[\"label\"]  # Get a seperate columns for labels\n",
        "news_test = fake_news_test.drop(\"label\", axis=1)\n",
        "news_test.set_index(\"id\", inplace=True)\n",
        "                                # all these operations are performed on test data chosen \n",
        "                                # above in the cell\n",
        "\n",
        "news_test[\"text\"] = news_test[\n",
        "    \"text\"\n",
        "].str.lower()  # convert the whole text to lower case to ensure uniformity\n",
        "\n",
        "news_test[\"text\"] = news_test[\"text\"].replace(\"[^A-Za-z\\s]\", \"\", regex=True)\n",
        "#Replace everything that is not letters or a space with a blank\n",
        "\n",
        "news_test[\"text\"] = news_test[\"text\"].str.split()  # split our text column to a list"
      ],
      "metadata": {
        "id": "XGt0P4XiVb7v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# make a stemmer and remove stopwords\n",
        "def remove_stopwords_and_stem(x):\n",
        "    stopwds_lst = stopwords.words(\"english\")\n",
        "    sentence = \"\"\n",
        "    for i in x:\n",
        "        if i in stopwds_lst:\n",
        "            x.remove(i)\n",
        "    for k in range(len(x)):\n",
        "        word = ps.stem(x[k])\n",
        "        x[k] = word\n",
        "    for j in x:\n",
        "        sentence = sentence + j + \" \"\n",
        "    sentence_final = sentence[:-1]\n",
        "    return sentence_final"
      ],
      "metadata": {
        "id": "-AZLHo4kVicI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47798be3-c482-4faf-e066-a87dc36f6617"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_test['text'] = news_test['text'].apply(remove_stopwords_and_stem) # apply the function"
      ],
      "metadata": {
        "id": "BP_WqXLWVguo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_comments = news_test['text'] \n",
        "# now we seperate the text into another series "
      ],
      "metadata": {
        "id": "4JGwE0MLVmSw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOG_test = Count_Vectorizer.transform(text_comments) # We pass test input to transform into BOG, \n",
        "                                                    # CV is fitted on train and transformed on test\n",
        "BOG_test = BOG_test.toarray() # seperate BOG(bag of words as a seperate array)\n",
        "label = np.array(label) # seperate labels as an array"
      ],
      "metadata": {
        "id": "yRTYxMR9Wc-T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = NaiveBayes_model.predict(BOG_test)\n",
        "from sklearn import metrics\n",
        "metrics.accuracy_score(label, predicted) # make prediction with Naivebayes\n",
        "metrics.recall_score(label, predicted) # make prediction with Naivebayes"
      ],
      "metadata": {
        "id": "vYvnwJmPWjYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f21f67-1695-4ecf-ee34-3fe72ef9c86f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8645973909131804"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_comments = list(news_test['text']) # make a list of all documents in the data\n",
        "one_hot_encoded = Tokenizer1.texts_to_sequences(text_comments)\n",
        "padded_vector = pad_sequences(one_hot_encoded, padding = 'post', maxlen= 500)\n",
        "input =np.array(padded_vector)\n",
        "output = np.array(label)  "
      ],
      "metadata": {
        "id": "_RsaQWnTWnQG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BiLSTM_Model.evaluate(input, output)"
      ],
      "metadata": {
        "id": "GgqbWxC4cIQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0da1a67-8af6-4259-cc06-d225fcae81f6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 39s 234ms/step - loss: 0.2532 - accuracy: 0.9338 - recall: 0.9186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2531854212284088, 0.9337514638900757, 0.9185785055160522]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_prob = BiLSTM_Model.predict(input, batch_size=64)"
      ],
      "metadata": {
        "id": "8UbMjUnNCOXH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_predicted = np.where(predicted_prob > 0.5, 1,0)"
      ],
      "metadata": {
        "id": "Sp4lGUmuCLUD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  print(classification_report(label, predicted))"
      ],
      "metadata": {
        "id": "y4iFIeSbce9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4620eca1-5ab7-48c2-ba85-aced42d40d51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91      2879\n",
            "           1       0.90      0.86      0.88      2223\n",
            "\n",
            "    accuracy                           0.90      5102\n",
            "   macro avg       0.90      0.89      0.90      5102\n",
            "weighted avg       0.90      0.90      0.90      5102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(label, keras_predicted))\n"
      ],
      "metadata": {
        "id": "vvdcY1mccwgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011f2e59-c62e-4b92-f679-27eb821b4853"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      2879\n",
            "           1       0.93      0.92      0.92      2223\n",
            "\n",
            "    accuracy                           0.93      5102\n",
            "   macro avg       0.93      0.93      0.93      5102\n",
            "weighted avg       0.93      0.93      0.93      5102\n",
            "\n"
          ]
        }
      ]
    }
  ]
}